{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Multi-Target Regression and SHAP Analysis\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class MultiTargetRegression:\n",
    "    def __init__(self, results_folder):\n",
    "        \"\"\"\n",
    "        Initialize the MultiTargetRegression class.\n",
    "        \n",
    "        Parameters:\n",
    "        - results_folder (str): Folder path to save results, plots, and models.\n",
    "        \"\"\"\n",
    "        self.results_folder = results_folder\n",
    "        # Ensure results folder exists\n",
    "        os.makedirs(self.results_folder, exist_ok=True)\n",
    "\n",
    "    def plot_true_vs_pred(self, y_true, y_pred, target):\n",
    "        \"\"\"\n",
    "        Plot True vs Predicted values for a specific target variable.\n",
    "        \n",
    "        Parameters:\n",
    "        - y_true (array): True values of the target variable.\n",
    "        - y_pred (array): Predicted values of the target variable.\n",
    "        - target (str): Name of the target variable.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(5, 5))  # Fixed size\n",
    "        sns.scatterplot(x=y_pred, y=y_true, alpha=0.3, s=15)  # Scatter plot\n",
    "        # Add a diagonal line representing perfect prediction\n",
    "        plt.plot([y_pred.min(), y_pred.max()], [y_pred.min(), y_pred.max()],\n",
    "                 color='red', linestyle='--', lw=1)\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.xlabel(\"Predicted Values\", fontsize=12)\n",
    "        plt.ylabel(\"True Values\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        # Save the plot as a PNG file\n",
    "        plot_path = os.path.join(self.results_folder, f\"true_vs_predicted_{target}.png\")\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_feature_importance(self, feature_importance_df):\n",
    "        \"\"\"\n",
    "        Plot SHAP-based feature importance for the top 15 features.\n",
    "        \n",
    "        Parameters:\n",
    "        - feature_importance_df (DataFrame): DataFrame containing features and their SHAP importance values.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(5, 3))  # Fixed size\n",
    "        sns.barplot(x=\"SHAP Importance\", y=\"Feature\", data=feature_importance_df.head(15))\n",
    "        plt.xticks(fontsize=8)\n",
    "        plt.yticks(fontsize=8)\n",
    "        plt.xlabel(\"SHAP Importance\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        # Save the plot as a PNG file\n",
    "        plot_path = os.path.join(self.results_folder, \"shap_top15_feature_importance.png\")\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_shap_summary(self, shap_values, X_test):\n",
    "        \"\"\"\n",
    "        Plot SHAP summary plot for feature contributions.\n",
    "        \n",
    "        Parameters:\n",
    "        - shap_values (array): SHAP values for the test dataset.\n",
    "        - X_test (DataFrame): Test dataset (features only).\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6, 5))  # Fixed size\n",
    "        shap.summary_plot(shap_values, X_test, show=False, plot_size=(6, 5))  # SHAP summary plot\n",
    "        plt.xticks(fontsize=8)\n",
    "        plt.yticks(fontsize=8)\n",
    "        plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize=10)\n",
    "        # Save the plot as a PNG file\n",
    "        plot_path = os.path.join(self.results_folder, \"shap_summary_plot.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "# Instantiate the MultiTargetRegression class\n",
    "results_folder = \"Regression analysis\"\n",
    "mt_regression = MultiTargetRegression(results_folder)\n",
    "\n",
    "# Step 1: Load the data\n",
    "# Load the input CSV file\n",
    "file_path = 'AI_datasets4.csv'\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Step 2: Define features and targets\n",
    "# Specify categorical and continuous feature columns\n",
    "categorical_features = ['Province', 'Food']\n",
    "continuous_features = [\n",
    "    'Urbanization_Rate', 'Population_Density', 'Per_Capita_GDP', 'Average_Elevation',\n",
    "    'Annual_Average_Temperature', 'Annual_Average_Rainfall', \"Industry_GDP\", 'Longitude', 'Latitude'\n",
    "]\n",
    "# Define the target variables (outputs)\n",
    "target_variables = ['TEQ_PCDDFs', 'TEQ_dlPCBs', 'TEQ_total', 'mPCBs_total', 'PBDEs_total']\n",
    "\n",
    "# Step 3: Define function to filter zeros and outliers\n",
    "def filter_outliers_and_zeros(df, target):\n",
    "    \"\"\"\n",
    "    Filter out rows with zero values and extreme outliers for a specific target variable.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): Input dataset.\n",
    "    - target (str): Name of the target variable.\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): Filtered dataset.\n",
    "    \"\"\"\n",
    "    df = df[df[target] != 0]  # Remove rows where target is zero\n",
    "    Q1, Q3 = df[target].quantile([0.25, 0.75])  # Calculate the first and third quartiles\n",
    "    IQR = Q3 - Q1  # Interquartile range\n",
    "    # Remove rows with values outside 2*IQR\n",
    "    df = df[(df[target] >= Q1 - 2 * IQR) & (df[target] <= Q3 + 2 * IQR)]\n",
    "    return df\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "# Convert categorical features into one-hot encoded features\n",
    "data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
    "# Filter outliers and zeros for each target variable\n",
    "for target in target_variables:\n",
    "    data = filter_outliers_and_zeros(data, target)\n",
    "# Drop rows with missing values in target variables\n",
    "data = data.dropna(subset=target_variables)\n",
    "# Save the processed dataset\n",
    "processed_data_path = os.path.join(results_folder, \"processed_data.csv\")\n",
    "data.to_csv(processed_data_path, index=False)\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X = data.drop(columns=target_variables)  # Features\n",
    "y = data[target_variables]  # Targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Step 6: Define and evaluate models\n",
    "# Define a dictionary of base models for evaluation\n",
    "base_models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"SVR\": SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "results = []  # Store results for each model\n",
    "best_model = None  # Placeholder for the best-performing model\n",
    "best_score = -np.inf  # Track the highest mean R² score\n",
    "model_performance = {}  # Store performance metrics for all models\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in base_models.items():\n",
    "    print(f\"Training and evaluating model: {name}\")\n",
    "    # Wrap the model in MultiOutputRegressor for multi-target regression\n",
    "    multi_output_model = MultiOutputRegressor(model)\n",
    "    # Perform cross-validation and calculate mean R² score\n",
    "    scores = cross_val_score(multi_output_model, X_train, y_train, scoring='r2', cv=5)\n",
    "    mean_r2 = np.mean(scores)\n",
    "    # Train the model on the training data\n",
    "    multi_output_model.fit(X_train, y_train)\n",
    "    # Make predictions on the test set\n",
    "    y_pred = multi_output_model.predict(X_test)\n",
    "    # Calculate test metrics\n",
    "    mse = mean_squared_error(y_test, y_pred, multioutput='uniform_average')\n",
    "    r2 = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "    # Save performance metrics\n",
    "    model_performance[name] = {\"R2\": r2, \"MSE\": mse}\n",
    "    results.append({\"Model\": name, \"Mean_R2\": mean_r2, \"Test_R2\": r2, \"Test_MSE\": mse})\n",
    "    # Update the best model if the current one performs better\n",
    "    if mean_r2 > best_score:\n",
    "        best_score = mean_r2\n",
    "        best_model = multi_output_model\n",
    "\n",
    "# Save the best model to a file\n",
    "best_model_path = os.path.join(results_folder, \"best_model.pkl\")\n",
    "joblib.dump(best_model, best_model_path)\n",
    "\n",
    "# Save the model comparison results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(results_folder, \"model_comparison_results.csv\"), index=False)\n",
    "\n",
    "# Step 7: Predictions and Visualization\n",
    "# Generate True vs Predicted plots for the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "for i, target in enumerate(target_variables):\n",
    "    mt_regression.plot_true_vs_pred(y_test.iloc[:, i], y_pred_best[:, i], target)\n",
    "\n",
    "# Step 8: SHAP Feature Importance\n",
    "# Use the first estimator in the best multi-output model\n",
    "best_estimator = best_model.estimators_[0]\n",
    "# Calculate SHAP values\n",
    "explainer = shap.Explainer(best_estimator, X_test)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Calculate and save SHAP feature importance\n",
    "shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"SHAP Importance\": shap_importance\n",
    "}).sort_values(by=\"SHAP Importance\", ascending=False)\n",
    "\n",
    "feature_importance_df.to_csv(os.path.join(results_folder, \"shap_feature_importance.csv\"), index=False)\n",
    "mt_regression.plot_feature_importance(feature_importance_df)\n",
    "mt_regression.plot_shap_summary(shap_values, X_test)\n",
    "\n",
    "print(\"Best model:\", best_model)\n"
   ],
   "id": "6951aa22a2131a82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Multi-Target Regression Analysis - Model Evaluation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, results_folder, target_variables):\n",
    "        \"\"\"\n",
    "        Initialize the ModelEvaluator class.\n",
    "        \n",
    "        Parameters:\n",
    "        - results_folder (str): Folder containing the trained model and processed dataset.\n",
    "        - target_variables (list): List of target variables to evaluate.\n",
    "        \"\"\"\n",
    "        self.results_folder = results_folder\n",
    "        self.target_variables = target_variables\n",
    "        self.evaluation_folder = os.path.join(results_folder, \"evaluation_results\")\n",
    "        # Create the evaluation folder if it doesn't exist\n",
    "        os.makedirs(self.evaluation_folder, exist_ok=True)\n",
    "\n",
    "    def load_model_and_data(self):\n",
    "        \"\"\"\n",
    "        Load the trained model and processed dataset.\n",
    "\n",
    "        Returns:\n",
    "        - model (object): Loaded trained model.\n",
    "        - data (DataFrame): Processed dataset.\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(self.results_folder, \"best_model.pkl\")\n",
    "        data_path = os.path.join(self.results_folder, \"processed_data.csv\")\n",
    "        try:\n",
    "            model = joblib.load(model_path)  # Load the model\n",
    "            data = pd.read_csv(data_path)  # Load the data\n",
    "            print(\"Model and data loaded successfully.\")\n",
    "            return model, data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model/data: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate regression metrics for model evaluation.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true (array): True target values.\n",
    "        - y_pred (array): Predicted target values.\n",
    "\n",
    "        Returns:\n",
    "        - metrics (dict): Dictionary containing RMSE, MAE, MedAE, and R².\n",
    "        \"\"\"\n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        medae = median_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        metrics = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"MedAE\": medae,\n",
    "            \"R2\": r2\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def calculate_q2(self, model, X, y):\n",
    "        \"\"\"\n",
    "        Calculate Q² (cross-validated R²) for the model.\n",
    "\n",
    "        Parameters:\n",
    "        - model (object): Trained model.\n",
    "        - X (DataFrame): Feature set.\n",
    "        - y (DataFrame): Target set.\n",
    "\n",
    "        Returns:\n",
    "        - mean_q2 (float): Average Q² across all targets.\n",
    "        - y_pred (array): Cross-validated predictions.\n",
    "        \"\"\"\n",
    "        y_pred = cross_val_predict(model, X, y, cv=5)  # Perform 5-fold cross-validation\n",
    "        q2_values = []\n",
    "        for i in range(y.shape[1]):  # Iterate over all target variables\n",
    "            y_true_col = y.iloc[:, i] if isinstance(y, pd.DataFrame) else y[:, i]\n",
    "            y_pred_col = y_pred[:, i]\n",
    "            ss_total = np.sum((y_true_col - np.mean(y_true_col)) ** 2)  # Total sum of squares\n",
    "            ss_residual = np.sum((y_true_col - y_pred_col) ** 2)  # Residual sum of squares\n",
    "            q2 = 1 - (ss_residual / ss_total)  # Calculate Q²\n",
    "            q2_values.append(q2)\n",
    "        return np.mean(q2_values), y_pred\n",
    "\n",
    "    def plot_true_vs_pred(self, y_true, y_pred, target):\n",
    "        \"\"\"\n",
    "        Plot True vs Predicted values for a specific target variable.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true (array): True values of the target variable.\n",
    "        - y_pred (array): Predicted values of the target variable.\n",
    "        - target (str): Name of the target variable.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.scatterplot(x=y_true, y=y_pred, alpha=0.7)\n",
    "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)  # Perfect prediction line\n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(f\"True vs Predicted for {target}\")\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(self.evaluation_folder, f\"true_vs_predicted_{target}.png\")\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_r2_q2_single(self, r2, q2, target):\n",
    "        \"\"\"\n",
    "        Plot R² vs Q² for a single target variable.\n",
    "\n",
    "        Parameters:\n",
    "        - r2 (float): R² value for the target variable.\n",
    "        - q2 (float): Q² value for the target variable.\n",
    "        - target (str): Name of the target variable.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter([r2], [q2], label=f\"{target}\", color=\"blue\", alpha=0.7)\n",
    "        plt.plot([r2], [r2], 'r--', lw=1, label=\"R² = Q² Line\")  # Reference line\n",
    "        plt.xlabel(\"R²\")\n",
    "        plt.ylabel(\"Q²\")\n",
    "        plt.title(f\"R² vs Q² for {target}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(self.evaluation_folder, f\"r2_q2_{target}.png\")\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"R²-Q² plot saved for {target} at {plot_path}\")\n",
    "\n",
    "    def permutation_test(self, model, X, y, n_permutations=100):\n",
    "        \"\"\"\n",
    "        Perform a permutation test to evaluate model significance.\n",
    "\n",
    "        Parameters:\n",
    "        - model (object): Trained model.\n",
    "        - X (DataFrame): Feature set.\n",
    "        - y (DataFrame): Target set.\n",
    "        - n_permutations (int): Number of permutations.\n",
    "\n",
    "        Returns:\n",
    "        - original_r2 (float): R² of the original model.\n",
    "        - permuted_r2_scores (list): R² scores of permuted models.\n",
    "        - p_value (float): P-value indicating the significance of the model.\n",
    "        \"\"\"\n",
    "        print(f\"Starting permutation test with {n_permutations} permutations...\")\n",
    "        original_r2 = r2_score(y, model.fit(X, y).predict(X))  # Original R² score\n",
    "        permuted_r2_scores = []\n",
    "\n",
    "        for _ in tqdm(range(n_permutations), desc=\"Permutations\"):\n",
    "            y_permuted = shuffle(y, random_state=None)  # Shuffle target values\n",
    "            permuted_r2 = r2_score(y_permuted, model.fit(X, y_permuted).predict(X))\n",
    "            permuted_r2_scores.append(permuted_r2)\n",
    "\n",
    "        # Calculate p-value as the proportion of permuted R² scores >= original R²\n",
    "        p_value = np.sum(np.array(permuted_r2_scores) >= original_r2) / n_permutations\n",
    "        print(f\"Permutation test completed. P-value: {p_value}\")\n",
    "        return original_r2, permuted_r2_scores, p_value\n",
    "\n",
    "    def evaluate_model(self, model, data):\n",
    "        \"\"\"\n",
    "        Perform full evaluation of the model, including metrics, R²-Q² plots, and permutation tests.\n",
    "\n",
    "        Parameters:\n",
    "        - model (object): Trained model.\n",
    "        - data (DataFrame): Processed dataset.\n",
    "        \"\"\"\n",
    "        X = data.drop(columns=self.target_variables)\n",
    "        y = data[self.target_variables]\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Calculate Q² and cross-validated predictions\n",
    "        q2, y_pred_cv = self.calculate_q2(model, X, y)\n",
    "        # Predict on the test set\n",
    "        y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "        metrics_list = []\n",
    "        for i, target in enumerate(self.target_variables):\n",
    "            y_test_target = y_test.iloc[:, i]\n",
    "            y_pred_target = y_pred[:, i]\n",
    "            y_cv_target = y_pred_cv[:, i]\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_metrics(y_test_target, y_pred_target)\n",
    "            metrics[\"Target\"] = target\n",
    "\n",
    "            # Calculate R² and Q²\n",
    "            r2 = metrics[\"R2\"]\n",
    "            q2 = r2_score(y.iloc[:, i], y_cv_target)\n",
    "            metrics[\"Q2\"] = q2\n",
    "\n",
    "            metrics_list.append(metrics)\n",
    "\n",
    "            print(f\"Metrics for {target}: {metrics}\")\n",
    "            # Plot True vs Predicted values\n",
    "            self.plot_true_vs_pred(y_test_target, y_pred_target, target)\n",
    "            # Plot R² vs Q²\n",
    "            self.plot_r2_q2_single(r2, q2, target)\n",
    "\n",
    "        # Save metrics to a CSV file\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        metrics_path = os.path.join(self.evaluation_folder, \"overall_metrics.csv\")\n",
    "        metrics_df.to_csv(metrics_path, index=False)\n",
    "        print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "        # Run permutation test\n",
    "        original_r2, permuted_r2_scores, p_value = self.permutation_test(model, X, y)\n",
    "        print(f\"Original R²: {original_r2:.4f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "        # Save permutation test results\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(permuted_r2_scores, kde=True, color=\"skyblue\", label=\"Permuted R²\")\n",
    "        plt.axvline(original_r2, color='red', linestyle='--', label='Original R²')\n",
    "        plt.xlabel(\"R² Scores\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Permutation Test Results\")\n",
    "        perm_test_path = os.path.join(self.evaluation_folder, \"permutation_test_results.png\")\n",
    "        plt.savefig(perm_test_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"Permutation test results saved to {perm_test_path}. Evaluation completed.\")\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Main method to load the model and data, and perform the evaluation.\n",
    "        \"\"\"\n",
    "        model, data = self.load_model_and_data()\n",
    "        if model is not None and data is not None:\n",
    "            self.evaluate_model(model, data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Folder containing the model and data\n",
    "    results_folder = \"multi_output_best_model_results1-5\"\n",
    "    # Target variables for multi-target regression\n",
    "    target_variables = ['TEQ_PCDDFs', 'TEQ_dlPCBs', 'TEQ_total', 'mPCBs_total', 'PBDEs_total']\n",
    "    # Initialize and run the evaluator\n",
    "    evaluator = ModelEvaluator(results_folder, target_variables)\n",
    "    evaluator.main()\n"
   ],
   "id": "8f6e22b22db4bd82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Individual regression model and SHAP analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pycaret.regression import setup, compare_models, tune_model, predict_model, save_model, pull\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Step 1: Load the data\n",
    "file_path = 'AI_datasets4.csv'  # Specify the path to the input dataset\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')  # Load the dataset with specified encoding\n",
    "\n",
    "# Step 2: Define feature groups\n",
    "# Define original categorical and continuous features\n",
    "original_categorical_features = ['Province', 'Food']\n",
    "continuous_features = [\n",
    "    'Urbanization_Rate', 'Population_Density', 'Per_Capita_GDP', 'Average_Elevation', \n",
    "    'Annual_Average_Temperature', 'Annual_Average_Rainfall', \"Industry_GDP\", 'Longitude', 'Latitude'\n",
    "]\n",
    "# Define the target variables for regression\n",
    "target_variables = ['TEQ_PCDDFs', 'TEQ_dlPCBs', 'TEQ_total', 'mPCBs_total', 'PBDEs_total']\n",
    "\n",
    "# Step 3: Preprocess data\n",
    "# Specify the folder to store results and ensure its existence\n",
    "results_folder = \"Regression analysis\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "def is_tunable(model):\n",
    "    \"\"\"\n",
    "    Check if the given model supports hyperparameter tuning in PyCaret.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Model object from PyCaret.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the model is tunable, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tune_model(model, n_iter=10, verbose=False)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def filter_outliers_and_zeros(df, target):\n",
    "    \"\"\"\n",
    "    Remove zero values and outliers from the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Input dataset.\n",
    "    - target (str): Target variable name.\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): Cleaned dataset.\n",
    "    \"\"\"\n",
    "    df = df[df[target] != 0]  # Remove rows where target value is zero\n",
    "    Q1, Q3 = df[target].quantile([0.25, 0.75])  # Calculate quartiles\n",
    "    IQR = Q3 - Q1  # Interquartile range\n",
    "    # Remove outliers based on 2*IQR\n",
    "    df = df[(df[target] >= Q1 - 2 * IQR) & (df[target] <= Q3 + 2 * IQR)]\n",
    "    return df\n",
    "\n",
    "def encode_and_map_features(data, categorical_features):\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding for categorical features.\n",
    "\n",
    "    Parameters:\n",
    "    - data (DataFrame): Input dataset.\n",
    "    - categorical_features (list): List of categorical feature names.\n",
    "\n",
    "    Returns:\n",
    "    - data (DataFrame): Dataset with encoded features.\n",
    "    - encoded_columns (list): List of encoded feature column names.\n",
    "    \"\"\"\n",
    "    data = pd.get_dummies(data, columns=categorical_features, drop_first=True)  # One-hot encoding\n",
    "    encoded_columns = [col for col in data.columns if col.startswith(tuple(categorical_features))]\n",
    "    return data, encoded_columns\n",
    "\n",
    "# Step 4: Model training and evaluation for each target variable\n",
    "# Store the best models and evaluation results for each target variable\n",
    "best_models = {}\n",
    "evaluation_results = []\n",
    "\n",
    "for target in target_variables:\n",
    "    print(f\"\\nProcessing target variable: {target}\")\n",
    "    sanitized_target = re.sub(r'[\\\\/*?:\"<>|]', '_', target)  # Sanitize target name for file paths\n",
    "\n",
    "    # Prepare data for the current target variable\n",
    "    data_target = data.dropna(subset=[target])  # Remove rows where the target is missing\n",
    "    data_target = filter_outliers_and_zeros(data_target, target)  # Remove zeros and outliers\n",
    "\n",
    "    # Drop other target variables to avoid data leakage\n",
    "    other_targets = [col for col in target_variables if col != target]\n",
    "    data_target = data_target.drop(columns=other_targets, errors='ignore')\n",
    "\n",
    "    # Encode categorical features\n",
    "    data_target, encoded_columns = encode_and_map_features(data_target, original_categorical_features)\n",
    "    categorical_features = encoded_columns\n",
    "\n",
    "    # Save the processed data for the current target\n",
    "    encoded_data_path = os.path.join(results_folder, f\"encoded_data_{sanitized_target}.csv\")\n",
    "    data_target.to_csv(encoded_data_path, index=False)\n",
    "    print(f\"Encoded data saved to {encoded_data_path}\")\n",
    "\n",
    "    # Setup PyCaret regression environment\n",
    "    regression_setup = setup(\n",
    "        data=data_target,\n",
    "        target=target,\n",
    "        categorical_features=categorical_features,\n",
    "        numeric_features=continuous_features,\n",
    "        session_id=42,  # Ensure reproducibility\n",
    "        normalize=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Compare and select the best models\n",
    "    best_model = compare_models(n_select=5)  # Select the top 5 models\n",
    "    # Tune the best model if tunable\n",
    "    tuned_model = tune_model(best_model[0]) if is_tunable(best_model[0]) else best_model[0]\n",
    "\n",
    "    # Save the tuned/best model\n",
    "    model_path = os.path.join(results_folder, f'best_model_{sanitized_target}.pkl')\n",
    "    save_model(tuned_model, model_path)\n",
    "    print(f\"Model for {target} saved to {model_path}\")\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    evaluation_table = pull()  # Get PyCaret's evaluation metrics\n",
    "    evaluation_table.to_csv(os.path.join(results_folder, f'evaluation_metrics_{sanitized_target}.csv'), index=False)\n",
    "    print(f\"Evaluation metrics for {target} saved.\")\n",
    "\n",
    "    # Save model predictions\n",
    "    predictions = predict_model(tuned_model, data=data_target)\n",
    "    predictions_file = os.path.join(results_folder, f'predictions_{sanitized_target}.csv')\n",
    "    pred_col = 'prediction' if 'prediction' in predictions.columns else 'prediction_label'\n",
    "    predictions[[target, pred_col]].to_csv(predictions_file, index=False)\n",
    "    print(f\"Predictions for {target} saved to {predictions_file}\")\n",
    "\n",
    "    # Generate True vs Predicted scatter plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=predictions[pred_col], y=predictions[target], alpha=0.6)\n",
    "    plt.plot([predictions[pred_col].min(), predictions[pred_col].max()],\n",
    "             [predictions[pred_col].min(), predictions[pred_col].max()],\n",
    "             color='red', linestyle='--', lw=2)\n",
    "    plt.title(f\"True vs Predicted for {target}\")\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('True Values')\n",
    "    plt.grid(True)\n",
    "    plot_path = os.path.join(results_folder, f'true_vs_predicted_{sanitized_target}.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"True vs Predicted plot saved for {target}.\")\n",
    "\n",
    "    # Feature Importance Analysis with SHAP\n",
    "    try:\n",
    "        import shap\n",
    "        # Create SHAP explainer and calculate SHAP values\n",
    "        explainer = shap.Explainer(tuned_model, data_target.drop(columns=[target], errors='ignore'))\n",
    "        shap_values = explainer(data_target.drop(columns=[target], errors='ignore'))\n",
    "\n",
    "        # Save feature importance to a CSV file\n",
    "        feature_importance = shap_values.abs.mean(0).values  # Average absolute SHAP values\n",
    "        feature_names = data_target.drop(columns=[target], errors='ignore').columns\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': feature_importance\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        feature_importance_path = os.path.join(results_folder, f\"feature_importance_{sanitized_target}.csv\")\n",
    "        feature_importance_df.to_csv(feature_importance_path, index=False)\n",
    "        print(f\"All feature importance saved to {feature_importance_path}\")\n",
    "\n",
    "        # Top 15 features for SHAP summary plot\n",
    "        top_features_idx = feature_importance.argsort()[-15:][::-1]\n",
    "        top_features = feature_names[top_features_idx]\n",
    "        shap_values_top = shap_values[:, top_features_idx]\n",
    "\n",
    "        # Generate and save SHAP summary plot\n",
    "        shap_summary_plot_path = os.path.join(results_folder, f\"shap_summary_top15_{sanitized_target}.png\")\n",
    "        shap.summary_plot(shap_values_top, data_target[top_features], show=False)\n",
    "        plt.savefig(shap_summary_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"SHAP summary plot for top 15 features saved to {shap_summary_plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance not available for {target}: {e}\")\n",
    "\n",
    "    # Store evaluation metrics and models\n",
    "    evaluation_table['Target'] = target\n",
    "    evaluation_results.append(evaluation_table)\n",
    "    best_models[target] = tuned_model\n",
    "\n",
    "# Combine all evaluation metrics into a single CSV\n",
    "combined_evaluation_path = os.path.join(results_folder, \"combined_evaluation_metrics.csv\")\n",
    "pd.concat(evaluation_results, ignore_index=True).to_csv(combined_evaluation_path, index=False)\n",
    "print(f\"\\nCombined evaluation metrics saved to {combined_evaluation_path}\")\n"
   ],
   "id": "897c75c757b9b4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation of individual regression model \n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, results_folder, target_variables):\n",
    "        \"\"\"\n",
    "        Initialize the ModelEvaluator class.\n",
    "        \n",
    "        Parameters:\n",
    "        - results_folder (str): The folder where the models and data files are stored.\n",
    "        - target_variables (list): List of target variables to evaluate.\n",
    "        \"\"\"\n",
    "        self.results_folder = results_folder\n",
    "        self.target_variables = target_variables\n",
    "        self.evaluation_folder = os.path.join(results_folder, \"evaluation_results2\")\n",
    "        \n",
    "        # Ensure that the evaluation results folder exists\n",
    "        os.makedirs(self.evaluation_folder, exist_ok=True)\n",
    "\n",
    "    def load_model_and_data(self, target):\n",
    "        \"\"\"\n",
    "        Load a pre-trained model and corresponding encoded data for a specific target variable.\n",
    "        \n",
    "        Parameters:\n",
    "        - target (str): The name of the target variable.\n",
    "\n",
    "        Returns:\n",
    "        - model (object): The loaded machine learning model.\n",
    "        - data (DataFrame): The encoded dataset corresponding to the target variable.\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(self.results_folder, f\"best_model_{target}.pkl.pkl\")\n",
    "        data_path = os.path.join(self.results_folder, f\"encoded_data_{target}.csv\")\n",
    "        try:\n",
    "            model = joblib.load(model_path)\n",
    "            data = pd.read_csv(data_path)\n",
    "            print(f\"Model and data loaded for {target}.\")\n",
    "            return model, data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model/data for {target}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate regression evaluation metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        - y_true (array): The true target values.\n",
    "        - y_pred (array): The predicted target values.\n",
    "\n",
    "        Returns:\n",
    "        - metrics (dict): A dictionary containing RMSE, MAE, and MedAE values.\n",
    "        \"\"\"\n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))  # Root Mean Squared Error\n",
    "        mae = mean_absolute_error(y_true, y_pred)       # Mean Absolute Error\n",
    "        medae = median_absolute_error(y_true, y_pred)   # Median Absolute Error\n",
    "        metrics = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"MedAE\": medae\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def permutation_test(self, model, X_test, y_test, n_permutations=50):\n",
    "        \"\"\"\n",
    "        Perform a permutation test to evaluate the robustness of the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - model (object): The machine learning model.\n",
    "        - X_test (DataFrame): The test features.\n",
    "        - y_test (array): The test target values.\n",
    "        - n_permutations (int): The number of permutations to perform.\n",
    "\n",
    "        Returns:\n",
    "        - original_score (float): The original R² score of the model.\n",
    "        - permuted_scores (list): A list of R² scores from permuted data.\n",
    "        \"\"\"\n",
    "        original_score = model.score(X_test, y_test)  # Original R² score\n",
    "        permuted_scores = []\n",
    "        for _ in tqdm(range(n_permutations), desc=\"Permutation Test\"):\n",
    "            # Shuffle the target values\n",
    "            y_test_permuted = np.random.permutation(y_test)\n",
    "            try:\n",
    "                # Calculate the model score on permuted data\n",
    "                permuted_score = model.score(X_test, y_test_permuted)\n",
    "            except Exception as e:\n",
    "                print(f\"Permutation test error: {e}\")\n",
    "                permuted_score = np.nan\n",
    "            permuted_scores.append(permuted_score)\n",
    "        return original_score, permuted_scores\n",
    "\n",
    "    def applicability_domain(self, X_train, X_test, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Assess the applicability domain using Mahalanobis distance.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train (DataFrame): The training features.\n",
    "        - X_test (DataFrame): The test features.\n",
    "        - epsilon (float): A small constant added for numerical stability.\n",
    "\n",
    "        Returns:\n",
    "        - mahalanobis_distances (array): Mahalanobis distances for the test samples.\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()  # Standardize features\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Calculate the covariance matrix and its pseudo-inverse\n",
    "        cov_matrix = np.cov(X_train_scaled, rowvar=False) + epsilon * np.eye(X_train_scaled.shape[1])\n",
    "        inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "\n",
    "        # Calculate Mahalanobis distances for each test sample\n",
    "        mahalanobis_distances = [\n",
    "            mahalanobis(x, np.mean(X_train_scaled, axis=0), inv_cov_matrix) for x in X_test_scaled\n",
    "        ]\n",
    "        return np.array(mahalanobis_distances)\n",
    "\n",
    "    def plot_true_vs_pred(self, y_true, y_pred, target):\n",
    "        \"\"\"\n",
    "        Create a scatter plot of true vs. predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true (array): The true target values.\n",
    "        - y_pred (array): The predicted target values.\n",
    "        - target (str): The name of the target variable.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        sns.scatterplot(x=y_pred, y=y_true, alpha=0.3, s=15)\n",
    "        plt.plot([y_pred.min(), y_pred.max()], [y_pred.min(), y_pred.max()],\n",
    "                 color='red', linestyle='--', lw=1)  # Line of perfect prediction\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.xlabel(\"Predicted Values\", fontsize=14)\n",
    "        plt.ylabel(\"True Values\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plot_path = os.path.join(self.evaluation_folder, f\"true_vs_predicted_{target}.png\")\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_permutation_test(self, original_score, permuted_scores, target):\n",
    "        \"\"\"\n",
    "        Plot the results of the permutation test.\n",
    "\n",
    "        Parameters:\n",
    "        - original_score (float): The original R² score.\n",
    "        - permuted_scores (list): A list of R² scores from permuted data.\n",
    "        - target (str): The name of the target variable.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        sns.histplot(permuted_scores, kde=True, bins=30, color='blue', alpha=0.5)\n",
    "        plt.axvline(original_score, color='red', linestyle='--', lw=1, label='Original Score')\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.xlabel(\"Permuted Scores\", fontsize=14)\n",
    "        plt.ylabel(\"Frequency\", fontsize=14)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(self.evaluation_folder, f\"permutation_test_{target}.png\")\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def evaluate_model(self, model, data, target):\n",
    "        \"\"\"\n",
    "        Perform the full evaluation for a single model and dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - model (object): The pre-trained machine learning model.\n",
    "        - data (DataFrame): The dataset corresponding to the target variable.\n",
    "        - target (str): The name of the target variable.\n",
    "        \"\"\"\n",
    "        X = data.drop(columns=[target], errors='ignore')  # Features\n",
    "        y = data[target]  # Target\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate and save evaluation metrics\n",
    "        metrics = self.calculate_metrics(y_test, y_pred)\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_path = os.path.join(self.evaluation_folder, f\"metrics_{target}.csv\")\n",
    "        metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "        # Generate evaluation plots\n",
    "        self.plot_true_vs_pred(y_test, y_pred, target)\n",
    "        original_score, permuted_scores = self.permutation_test(model, X_test, y_test)\n",
    "        self.plot_permutation_test(original_score, permuted_scores, target)\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Iterate through all target variables and evaluate their models.\n",
    "        \"\"\"\n",
    "        for target in self.target_variables:\n",
    "            model, data = self.load_model_and_data(target)\n",
    "            if model is not None and data is not None:\n",
    "                self.evaluate_model(model, data, target)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the results folder and target variables\n",
    "    results_folder = \"Regression analysis\"\n",
    "    target_variables = ['TEQ_PCDDFs', 'TEQ_dlPCBs', 'TEQ_total', 'mPCBs_total', 'PBDEs_total']\n",
    "\n",
    "    # Initialize and run the ModelEvaluator\n",
    "    evaluator = ModelEvaluator(results_folder, target_variables)\n",
    "    evaluator.main()\n"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
